# This is a comprehensive example file for i18n-rules.yml.
# It documents all available configuration options for the localization tool.
# Version 2: Corrected and updated with all parameters.

# -----------------------------------------------------------------------------
# 1. Language Settings
# -----------------------------------------------------------------------------
# Defines the source language of your files and the target languages for translation.
languages:
  # The source language code (e.g., 'en', 'ja', 'de').
  source: en

  # A list of language codes to translate the source text into.
  targets:
    - zh-Hant
    - ja
    - fr

  # (Optional) A map to convert standard language codes to API-specific ones.
  # This is useful if a translation provider uses a different code for a language.
  # The key is the language code from the 'targets' list, and the value is the API-specific code.
  map:
    zh-Hant: "zh" # Example for DeepL which uses "zh" for Traditional Chinese.

# -----------------------------------------------------------------------------
# 2. File Processing Rules
# -----------------------------------------------------------------------------
# A list of rules that define which files to process, how to extract strings,
# and which translator to use.
file_rules:
  # --- Rule Example 1: OpenAI Provider ---
  - name: "Shell Script Comments (using OpenAI)"
    file_mapping:
      "sh/utilkit.src.sh": "sh/utilkit.i18n.sh"
    extraction_rules:
      - pattern: "# (.+)"
        capture_group: 1

    translator:
      provider: "openai"
      # (Optional) Manually set the number of texts to process in a single API call.
      # If you set this, auto-calculation is disabled. If unset, defaults to 100.
      batch_size: 50
      # (Optional) Your OpenAI API key. It's recommended to use the
      # OPENAI_API_KEY environment variable instead for better security.
      # api_key: "sk-..."
      # (Optional) The base URL for the OpenAI API. Useful for proxying or using
      # compatible services like LiteLLM.
      # base_url: "https://api.example.com/v1"
      model: "gpt-4o-mini"
      temperature: 0.1
      # --- Settings for Auto Batch Size Calculation ---
      # The following settings are used to auto-calculate batch_size if it's not set manually.
      # (Optional) Rate limiting: Requests Per Minute.
      rpm: 60
      # (Optional) Rate limiting: Tokens Per Minute.
      tpm: 250000
      # (Optional) An estimate of the average number of tokens per text segment.
      avg_tokens_per_text: 50
      # (Optional) Provide surrounding text as context to improve translation quality.
      context:
        # Number of preceding strings to include as context.
        before: 2
        # Number of succeeding strings to include as context.
        after: 2
      # (Optional) Custom prompts for the OpenAI API.
      # The system now supports separate prompts for batch translation (current)
      # and single-text translation (when batch_size is 1).
      # Use {{targ_lang}} for the target language and {{contxt}} for the content.
      prompts:
        # Prompts for batch translation (multiple texts in a JSON array)
        system: "You are a localization expert. Translate the JSON array of text segments provided by the user. Follow all rules precisely."
        user: "Translate the text segments in the following JSON array to {{targ_lang}}.\n\n{{contxt}}"
        # Prompts for single text translation (a plain text string)
        single_system: "You are a localization expert. Translate the single text segment provided by the user."
        single_user: "Translate the following text to {{targ_lang}}.\n\n{{contxt}}"

  # --- Rule Example 2: Gemini Provider (Corrected Structure) ---
  - name: "Markdown Headers (using Gemini)"
    file_mapping:
      "README.md": "README.i18n.md"
    extraction_rules:
      - pattern: "^## (.+)"
        capture_group: 1
    translator:
      provider: "gemini"
      # (Optional) Manually set batch size. If you set this, auto-calculation is disabled. If unset, defaults to 100.
      batch_size: 40
      # (Optional) Your Gemini API key. It's recommended to use the
      # GEMINI_API_KEY environment variable instead.
      # api_key: "..."
      model: "gemini-1.5-flash"
      temperature: 0.1
      # --- Settings for Auto Batch Size Calculation ---
      # The following settings are used to auto-calculate batch_size if it's not set manually.
      # (Optional) Rate limiting: Requests Per Minute.
      rpm: 30
      # (Optional) Rate limiting: Tokens Per Minute.
      tpm: 250000
      # (Optional) An estimate of the average number of tokens per text segment.
      avg_tokens_per_text: 50
      # (Optional) Custom prompts for the Gemini API.
      # Supports 'batch' for JSON array processing and 'single' for plain text.
      # Note: Gemini provider uses {{src_lang}} and {{targ_lang}} placeholders.
      prompts:
        batch: "You are a localization expert. Translate the JSON array of text segments to {{targ_lang}}.\n\n{{contxt}}"
        single: "You are a localization expert. Translate the following text to {{targ_lang}}.\n\n{{contxt}}"

  # --- Rule Example 3: DeepL Provider (via deep-translator) ---
  - name: "JSON Values (using DeepL)"
    file_mapping:
      "en.json": "template.json"
    extraction_rules:
      - pattern: '\"value\": \"(.+)\"'
        capture_group: 1
    translator:
      provider: "deepl"
      # (Optional) Manually set the number of texts to process in a single API call.
      # Defaults to 100.
      batch_size: 100
      # For deep-translator providers, API keys and other options are passed directly.
      # It's recommended to use environment variables where possible (e.g., DEEPL_API_KEY).
      api_key: "YOUR_DEEPL_API_KEY" # Replace with your key or set env var

  # --- Rule Example 4: Google Translate (via deep-translator) ---
  - name: "Text file content (using Google Translate)"
    file_mapping:
      "source.txt": "template.txt"
    extraction_rules:
      - pattern: "(.+)" # Capture every line
        capture_group: 1
    translator:
      provider: "google"
      # (Optional) Manually set the number of texts to process in a single API call.
      # Defaults to 100.
      batch_size: 100
      # 'google' is one of the default providers in 'deep-translator'.
      # It does not require an API key for basic use.

  # --- Rule Example 5: Mock Provider ---
  - name: "Test Rule (using Mock Translator)"
    file_mapping:
      "test.txt": "test.i18n.txt"
    extraction_rules:
      - pattern: "(.+)"
        capture_group: 1
    translator:
      provider: "mock"
      # (Optional) Manually set the number of texts to process in a single API call.
      # Defaults to 100.
      batch_size: 100
      # The mock translator is for testing the extraction and compilation process
      # without making real API calls. It returns the original text with the
      # target language code appended (e.g., "Hello World_zh-Hant").

  # --- Rule Example 6: Ollama Provider ---
  - name: "Local LLM Test (using Ollama)"
    file_mapping:
      "local_test.txt": "local_test.i18n.txt"
    extraction_rules:
      - pattern: "(.+)"
        capture_group: 1
    translator:
      provider: "ollama"
      # (Required) The model to use with your local Ollama instance.
      model: "llama3"
      # (Optional) The host URL for your Ollama service.
      # Defaults to "http://localhost:11434".
      host: "http://localhost:11434"
      # (Optional) Manually set batch size. Defaults to 100.
      batch_size: 20
      # (Optional) Custom prompts for the Ollama API, similar to OpenAI.
      prompts:
        system: "You are a localization expert. Translate the JSON array of text segments provided by the user."
        user: "Translate the text segments in the following JSON array to {{targ_lang}}.\n\n{{contxt}}"
        single_system: "You are a localization expert. Translate the single text segment provided by the user."
        single_user: "Translate the following text to {{targ_lang}}.\n\n{{contxt}}"
# --- Global Debug Flag ---
# The 'debug' flag is not set here directly. It is passed from the command line
# when you run the script: `python -m scripts.translator.main --debug`
# When enabled, it provides verbose logging for all rules, including API
# requests and responses, which is useful for troubleshooting.
